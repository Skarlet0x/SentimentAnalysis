{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongTextSentiment_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "CNEsZpJJ1CrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "SJ4JM14fc3sp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R9GdS9A1HYgo"
      },
      "outputs": [],
      "source": [
        "# using torch because there is no pretrained model on HuggingFace for TextClassification that is trained on an IMDB dataset for tf \n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRETRAINED MODEL"
      ],
      "metadata": {
        "id": "a3dGxDDjc4yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE: https://huggingface.co/aychang/roberta-base-imdb\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aychang/roberta-base-imdb\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"aychang/roberta-base-imdb\")"
      ],
      "metadata": {
        "id": "GeNHLbJ0zwUJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEVELOPMENT"
      ],
      "metadata": {
        "id": "KhO5yIbQczNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE: https://www.denofgeek.com/tv/star-trek-discovery-season-4-episode-8-review-all-in/\n",
        "text = \"\"\"It’s been a month since Star Trek: Discovery has been on our screens, so you’d be forgiven for hoping that the series’ return from its midseason break might at last offer up some answers about the big Season 4 mystery surrounding the Dark Matter Anomaly and the mysterious Species 10-C that created it. Or at least move Season 4’s overall plot along a bit more deliberately than this particular hour manages to do. Instead, “All In” is a decent enough hour in a world where we haven’t had a new episode in several weeks, but one that’s more focused on Michael and Book’s relationship and whether it will be able to survive his decision to embrace Ruon Tarka’s plan to destroy the DMA.\n",
        "\n",
        "The last-minute revelation that Species 10-C is some sort of highly evolved race with dangerously advanced technological capabilities that are using the DMA to dredge for the tiny energy-rich elements needed to power its vast Faraday cage-style space dome is intriguing. (And lends credence to the Federation’s fear that attempting to destroy the DMA–and thereby damage Species 10-C’s power supply—could lead to a war no one wants or is ready to fight.) But it all feels a bit tacked on to the rest of the hour, and one can only hope we’ll get to delve into all this in a more substantive way during next week’s installment.\n",
        "\n",
        "In all honesty, a lot of “All In” feels as though it’s made of moments that might have been better served in different episodes: Hugh Culber’s stress breakdown that gets just a single scene to play out, Michael’s sudden interest in Owo’s mental state even though the two have barely spoken this season, and the revelation of the lieutenant commander’s apparent cage-fighting skills, just for a start. Yet, the hour’s criminal underworld setting and vague heist vibes are a welcome reprieve from the recent run of Discovery episodes that have largely tended to involve people arguing around a Federation Headquarters’ desk as much as I have enjoyed the depth of their conversations. (Speaking of which, are we just…never seeing Tilly again? Even though Discovery is constantly at what is essentially home base?)\n",
        "\n",
        "As most of us likely expected, the bulk of this episode focuses on the aftermath of Book’s decision to steal the prototype spore drive and head off to try and destroy the DMA on his own. Michael, naturally, is torn between her love for her partner and her duty to the Federation—whose inclination to diplomacy she fully supports in this particular instance. Though both Admiral Vance and President Rillak tell her to keep out of it, as she is obviously too close to what’s happening to be involved, Vance counts on her not listening to those instructions and manages to give her a secondary mission—tracking down some star charts from a less advanced species whose homeworld is basically across the galactic barrier from where Species 10-C exists—that allows her to go after Book anyway. \n",
        "\n",
        "I keep thinking that at some point, I’m going to get over how annoying it is that Michael has bent and broken the rules so often that it’s now not only expected of her but behavior that is actively encouraged, and yet. Here I am! To be fair, Season 4 has been remarkably good at delineating the differences between Michael Burnham, Starfleet officer, and Michael Burnham, Starfleet captain, and what her change in station has meant for the way she’s had to approach problems and manage competing loyalties.\n",
        "\n",
        "But so much of this episode feels like backsliding on that front, and suddenly we’re right back to Michael doing things she shouldn’t and getting rewarded for them in the end. Even her admission at the end of the hour that she’ll have to do whatever’s necessary to stop Book and Tarka from destroying the DMA rings more hollow than it would have before this episode when it at least felt like Discovery was trying to attach real stakes and weight and even some level of accountability to Michael’s decisions as captain. (At last!)\n",
        "\n",
        "But now — Does anyone really think she’d kill Book to avert a war rather than come up with an insane scheme to stop him that eventually, Rillak will somehow have to dub brilliant? Don’t get me wrong, I don’t want anything to happen to Book (David Ajala remains my Season 4 MVP), but please let there at least be some real consequences attached to whatever Michael’s about to do to save him. Even if those consequences come from Book in some way.\n",
        "\n",
        "One of the best elements about this episode, though, is the way it calls back, both in setting and tone, to Michael and Book’s first adventures together when she came to the 32nd century. It’s easy to forget that the two of them spent a year together as couriers when Michael thought she’d lost her Starfleet life forever, working with shady types and transporting illicit materials all over the galaxy. The ease with which they slip back into that partnership, from their awkwardly fake banter to the clear hand singles they use with one another, is a strangely welcome and necessary reminder of how good they really are together, something that has been (rightfully I think) overshadowed by Book’s grief over Kwaijon. \n",
        "\n",
        "Which, of course, comes at the precise moment where their future together seems more clouded and uncertain than ever. As is typical of the mature way Discovery has always written the romance between these two, the ultimate fate of Book and Burnham’s relationship has little  to do with how much they love one another. Instead, it’s about whether they’ll be able to successfully navigate what appear to be rather significant differences in moral philosophy. Book is willing to risk it all to prevent other races from feeling the loss he now struggles with every day and embraces an end justify the means attitude toward protecting those lives. \n",
        "\n",
        "But Burnham, a child of the Federation through and through, believes that diplomacy, leading with hope, and showing our best face to those that are different from us must be the first path that humanity always tries, even with those who may not deserve that particular bit of grace. How they will find a common ground in the middle of that, when Book’s personal experiences with the DMA and Species 10-C make him especially certain of the rightness of his perspective, is anyone’s guess. Despite Michael’’s clear willingness to fight for Book, here’s certainly more than one moment in “All In” where it seems as though the pair are gambling on more than a card game, but the very future of their relationship and it doesn’t feel like either of them really win. (But props to Burnham for understanding that no matter how much she might have wanted to believe she could change his mind, she still needed to plant that tracking device anyway.)\"\"\""
      ],
      "metadata": {
        "id": "XoaJIJYAzz0x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not add special tokens now - LATER, manually\n",
        "tokens = tokenizer.encode_plus(text, add_special_tokens=False, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "M3yMnVlQ0mWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b48ee5b-fae7-4406-ac66-a7209474c2be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1490 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens[\"input_ids\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtxRLfzd033x",
        "outputId": "f60b8145-8f70-484a-b4fb-f736249bed0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1490"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 512\n",
        "\n",
        "# 510 sized chunks (because of two BERT special tokens which will be added later)\n",
        "\n",
        "input_id_chunks = list(tokens[\"input_ids\"][0].split(chunk_size - 2))\n",
        "mask_chunks = list(tokens[\"attention_mask\"][0].split(chunk_size - 2))"
      ],
      "metadata": {
        "id": "FhV6uEvG2j-f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_total = len(input_id_chunks)"
      ],
      "metadata": {
        "id": "zz6MTsON5OCl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1490 / 512 = 2.91\n",
        "chunks_total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJGjvpIB5X1F",
        "outputId": "76e8115d-a9ab-46b9-b1f1-52abf6c71657"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking the text\n",
        "\n",
        "for i in range(chunks_total):\n",
        "    # CLS and SEP tokens\n",
        "    input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "\n",
        "    # attention tokens\n",
        "    mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "\n",
        "    # get padding length (difference between 512 and actual embedding length)\n",
        "    pad_len = chunk_size - input_id_chunks[i].shape[0]\n",
        "\n",
        "    if pad_len > 0:\n",
        "        # if padding length is positive (the embedding is shorter than 512), apply padding [0] in the amount of this difference\n",
        "        input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "        mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])"
      ],
      "metadata": {
        "id": "Gqs9i4mI296G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if all chunks are the same length\n",
        "for chunk in input_id_chunks:\n",
        "    print(len(chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XCCdhTu4Ntu",
        "outputId": "dd925bc0-0d2a-49c8-977d-cb14600c278e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "512\n",
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check our result - start with 101, ends with 102, padded to 512\n",
        "chunk.long()"
      ],
      "metadata": {
        "id": "NlDnB6pt4P7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0a77fe-9e10-4d80-943a-d8da837a8a64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,     8,  6328,     6,     7,   988,     8,  5972,    17,    27,\n",
              "           29,    78, 18848,   561,    77,    79,   376,     7,     5,  2107,\n",
              "         1187,  3220,     4,    85,    17,    27,    29,  1365,     7,  4309,\n",
              "           14,     5,    80,     9,   106,  1240,    10,    76,   561,    25,\n",
              "        15763, 21724,    77,   988,   802,    79,    17,    27,   417,   685,\n",
              "           69, 47877,   301,  6000,     6,   447,    19, 31665,  3505,     8,\n",
              "        21778, 16108,  3183,    70,    81,     5, 22703,     4,    20,  5136,\n",
              "           19,    61,    51,  9215,   124,    88,    14,  3088,     6,    31,\n",
              "           49, 34556,  4486, 32723,     7,     5,   699,   865,  7695,    51,\n",
              "          304,    19,    65,   277,     6,    16,    10, 34374,  2814,     8,\n",
              "         2139,  8306,     9,   141,   205,    51,   269,    32,   561,     6,\n",
              "          402,    14,    34,    57,    36,  4070,  6459,    38,   206,    43,\n",
              "        22140,    30,  5972,    17,    27,    29, 12903,    81, 11947,  1439,\n",
              "        23368,     4,  1437, 50118, 50118, 32251,     6,     9,   768,     6,\n",
              "          606,    23,     5, 12548,  1151,   147,    49,   499,   561,  1302,\n",
              "           55,  3613,   196,     8,  9684,    87,   655,     4,   287,    16,\n",
              "         6097,     9,     5, 11374,   169, 14735,    34,   460,  1982,     5,\n",
              "         9884,   227,   209,    80,     6,     5,  7017,  7658,     9,  5972,\n",
              "            8,  7960,  1908,    17,    27,    29,  1291,    34,   410,  1437,\n",
              "            7,   109,    19,   141,   203,    51,   657,    65,   277,     4,\n",
              "         2978,     6,    24,    17,    27,    29,    59,   549,    51,    17,\n",
              "           27,   890,    28,   441,     7,  5116, 11821,    99,  2082,     7,\n",
              "           28,  1195,  1233,  5550,    11,  7654, 10561,     4,  5972,    16,\n",
              "         2882,     7,   810,    24,    70,     7,  2097,    97,  4694,    31,\n",
              "         2157,     5,   872,    37,   122,  6952,    19,   358,   183,     8,\n",
              "        28978,    41,   253, 11071,     5,   839,  6784,  1706,  6244,   167,\n",
              "         1074,     4,  1437, 50118, 50118,  1708,  7960,  1908,     6,    10,\n",
              "          920,     9,     5,  6692,   149,     8,   149,     6,  2046,    14,\n",
              "        15166,     6,   981,    19,  1034,     6,     8,  2018,    84,   275,\n",
              "          652,     7,   167,    14,    32,   430,    31,   201,   531,    28,\n",
              "            5,    78,  2718,    14,  9187,   460,  5741,     6,   190,    19,\n",
              "          167,    54,   189,    45,  6565,    14,  1989,   828,     9, 11564,\n",
              "            4,  1336,    51,    40,   465,    10,  1537,  1255,    11,     5,\n",
              "         1692,     9,    14,     6,    77,  5972,    17,    27,    29,  1081,\n",
              "         3734,    19,     5,   211,  5273,     8, 33797,   158,    12,   347,\n",
              "          146,   123,   941,  1402,     9,     5,   235,  1825,     9,    39,\n",
              "         4263,     6,    16,  1268,    17,    27,    29,  4443,     4,  2285,\n",
              "          988,    17,    27,    17,    27,    29,   699, 10640,     7,  1032,\n",
              "           13,  5972,     6,   259,    17,    27,    29,  1819,    55,    87,\n",
              "           65,  1151,    11,    44,    48,  3684,    96,    17,    46,   147,\n",
              "           24,  1302,    25,   600,     5,  1763,    32, 11008,    15,    55,\n",
              "           87,    10,  1886,   177,     6,    53,     5,   182,   499,     9,\n",
              "           49,  1291,     8,    24,   630,    17,    27,    90,   619,   101,\n",
              "         1169,     9,   106,   269,   339,     4,    36,  1708, 26504,     7,\n",
              "         7960,  1908,    13,  2969,    14,   117,   948,   141,   203,    79,\n",
              "          429,    33,   770,     7,   679,    79,   115,   464,    39,  1508,\n",
              "            6,    79,   202,   956,     7,  2195,    14,  6779,  2187,  6992,\n",
              "         1592,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stack everything together\n",
        "input_ids = torch.stack(input_id_chunks).long()\n",
        "attention_mask = torch.stack(mask_chunks).int()"
      ],
      "metadata": {
        "id": "YITuQKfh63A8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiLfWiE58Zgs",
        "outputId": "53e97f6c-ba42-47f7-acb7-6264e9ec161e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101,  243,   17,  ..., 9123,    9,  102],\n",
              "        [ 101, 5972,   17,  ...,   11, 2749,  102],\n",
              "        [ 101,    8, 6328,  ...,    0,    0,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJntMKjk8avT",
        "outputId": "7c6b4aeb-1387-4786-c893-14711c43274c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reformat\n",
        "input_dict = {\"input_ids\": input_ids,\n",
        "              \"attention_mask\": attention_mask}"
      ],
      "metadata": {
        "id": "g-AZp3XC8YSE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZEZYz6Q-VVq",
        "outputId": "2a23321d-da41-49d0-ff93-7a615931e9cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32),\n",
              " 'input_ids': tensor([[ 101,  243,   17,  ..., 9123,    9,  102],\n",
              "         [ 101, 5972,   17,  ...,   11, 2749,  102],\n",
              "         [ 101,    8, 6328,  ...,    0,    0,    0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# roBERTa\n",
        "outputs = model(**input_dict)"
      ],
      "metadata": {
        "id": "Bg3ydsjS7Bnc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRXoCLmV8l4T",
        "outputId": "e43e29bf-fae5-4970-8803-d49e8992426d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput([('logits', tensor([[-2.7947,  2.7715],\n",
              "                                   [ 2.2441, -2.0543],\n",
              "                                   [-3.3942,  3.4687]], grad_fn=<AddmmBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)"
      ],
      "metadata": {
        "id": "bCkUS1VL8kvr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5tEO7I2ESsW",
        "outputId": "e1cb8480-f8de-4833-b465-e9ebf0757e4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0038, 0.9962],\n",
              "        [0.9866, 0.0134],\n",
              "        [0.0010, 0.9990]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_probs = probabilities.mean(dim=0)"
      ],
      "metadata": {
        "id": "ENxEnKDo7Dib"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = torch.argmax(avg_probs).item()"
      ],
      "metadata": {
        "id": "qrHfr5fBAMgh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - negative\n",
        "# 1 - positive\n",
        "\n",
        "# the reviewer gives it a 3.6/5, so I guess it's correct\n",
        "sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_g39eh0BSwJ",
        "outputId": "eaf88dac-caa4-44b7-a1a9-4a082968d4f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FUNCTIONS AND NEW DATA"
      ],
      "metadata": {
        "id": "ZYTbe0HecvYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def long_text_tensors(text):\n",
        "  tokens = tokenizer.encode_plus(text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  chunk_size = 512\n",
        "\n",
        "  input_id_chunks = list(tokens[\"input_ids\"][0].split(chunk_size - 2))\n",
        "  mask_chunks = list(tokens[\"attention_mask\"][0].split(chunk_size - 2))\n",
        "\n",
        "  chunks_total = len(input_id_chunks)\n",
        "\n",
        "  for i in range(chunks_total):\n",
        "    input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "    mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "    pad_len = chunk_size - input_id_chunks[i].shape[0]\n",
        "\n",
        "    if pad_len > 0:\n",
        "        input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "        mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "\n",
        "  input_ids = torch.stack(input_id_chunks)\n",
        "  attention_mask = torch.stack(mask_chunks)\n",
        "\n",
        "  input_dict = {\"input_ids\": input_ids.long(),\n",
        "              \"attention_mask\": attention_mask.int()}\n",
        "\n",
        "  return input_dict\n",
        "\n",
        "\n",
        "def predict_sentiment(input_dict):\n",
        "    outputs = model(**input_dict)\n",
        "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
        "    avg_probs = probabilities.mean(dim=0)\n",
        "    sentiment = torch.argmax(avg_probs).item()\n",
        "\n",
        "    if sentiment == 0:\n",
        "      return \"NEGATIVE\"\n",
        "\n",
        "    elif sentiment == 1:\n",
        "      return \"POSITIVE\""
      ],
      "metadata": {
        "id": "KlZ6GySNEqO3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE: https://www.imdb.com/review/rw3814011/?ref_=tt_urv\n",
        "txt = (\"\"\"Where, o where is the Zero rating that IMDB needs to add? This would certainly rate it. This absolute trash of a show takes fifty years of Star Trek (I've been watching since 1966) and flushes it down the toilet. There wasn't one episode worth watching in the entire first season. Not one. Let's see what's bad about this show.\n",
        "\n",
        "1 - It seems that the show runners/writers know nothing about Star Trek. If they did, this wouldn't have been created. They also lied, claiming that this takes place in the real or original time line. No way.\n",
        "\n",
        "2 - Here's the biggest question: If this is the prime time line, where has Spock's 'adopted human' sister been for FIFTY YEARS? In a coma? An alternate reality? Was he hiding her from Kirk? This is one of the absolute dumbest ideas ever. So because she was raised on Vulcan, does that make her able to be a Vulcan?\n",
        "\n",
        "3 - Designs. The Discovery is one of the ugliest ships ever. It should be a Klingon ship. And since when to Federation ships have golden hulls? Which pinhead designed those horrible uniforms that look like they'd be rejected at Studio 54! As for the Klingon, Glenn Hetrick should be ashamed. Where the hell did they come from? They look like the only warring they would do is under a disco ball on a dance floor.\n",
        "\n",
        "4 - Continuity. Let's just toss that to the winds. So the Federation knew about the Mirror universe at least ten years before Kirk discovered it? Why wasn't this in the databanks? Oh, we just wanted things to be cool. Yeah. Ok.\n",
        "\n",
        "5 - the cast. Sorry but the guy playing the doctor looks WAY too young for the role. Perhaps he he were an alien, that might be different. Doug Jones is great, but the character he portrays is one of the sillier aliens. Oh, he can sense death can he? So can I - just put someone in a red shirt. Oh that's right - everyone's in those ridiculous disco uniforms.\n",
        "\n",
        "6 - I know Bryan Fuller thinks it's cool (what an insipid concept) to give women Male names. Cool isn't a good enough reason. How about she's named after a relative who died in the service? See how simple?\n",
        "\n",
        "This show is nothing but an obscene money grab for CBS. The sad part is, that those deluded fans who say \"All Trek is Good Trek\" may keep this alive for a while. I won't be one of them. I like quality writing, something this show is missing.\"\"\")"
      ],
      "metadata": {
        "id": "-S5P4WIuGW2l"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dict = long_text_tensors(txt)"
      ],
      "metadata": {
        "id": "5Dtt7uRjGvA-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(input_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "05omrSOWJOUE",
        "outputId": "8fb637fc-96be-4630-eeac-dc32b61eba22"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NEGATIVE'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}